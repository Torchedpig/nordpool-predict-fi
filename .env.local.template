# Fingrid API key for nuclear power production data
FINGRID_API_KEY="your_fingrid_api_key"

# ENTSO-E API key for nuclear downtime announcements
ENTSO_E_API_KEY="your_entsoe_api_key"

# API key(s) for generating text with the --narrate option
LLM_API_KEY="your_deepseek_api_key" # or OpenAI, Gemini, etc.
LLM_MODEL=deepseek-chat # or gpt-4o, Gemini, Llama, etc.
LLM_API_BASE=https://api.deepseek.com/v1/ # "https://api.openai.com/v1/"

# Ollama LLM support (for local LLMs)
# Set OLLAMA_API_BASE to your Ollama server, e.g. http://localhost:11434
OLLAMA_API_BASE="http://localhost:11434"
OLLAMA_MODEL="llama3"

# vLLM LLM support (OpenAI-compatible endpoints)
# Set VLLM_API_BASE to your vLLM server, e.g. http://localhost:8000/v1
VLLM_API_BASE="http://localhost:8000/v1"
VLLM_MODEL="your-vllm-model-name"
VLLM_MAX_TOKENS=1536

# Directory paths for data and deployment
DATA_FOLDER_PATH="data"
DEPLOY_FOLDER_PATH="deploy"
LOG_FOLGER_PATH="logs"
ARCHIVE_FOLDER_PATH="archive"

# Local SQLite database file path
DB_PATH="data/prediction.db"

# Filenames to be placed in the deployment folder
PREDICTIONS_FILE="prediction.json"
AVERAGES_FILE="averages.json"
NARRATION_FILE="narration.md"

# FMI station IDs for wind speed measurements (comma-separated)
FMISID_WS="101784,101673,101661,101783,101846,101464,101481,101785,101794,101660,101256,101268,101485,101462,101061,101267,101840,100932,100908,101851"

# FMI station IDs for temperature measurements (comma-separated)
FMISID_T="101784,101673,101661,101783,101846,101464,101481,101785,101794,101660,101256,101268,101485,101462,101061,101267,101840,100932,100908,101851"

# Delay time in seconds between script runs (default: 3600)
DELAY_TIME=3600

# Options to pass to nordpool_predict_fi.py (e.g. --predict --narrate --commit --deploy)
# You can enable/disable features by editing this line.
PREDICT_OPTS=--predict --narrate --commit --deploy

# Port for the web service (static file server)
WEB_PORT=8500

# LLM provider selection: 'openai' (OpenAI/DeepSeek), 'ollama', or 'vllm' (OpenAI-compatible vLLM endpoints)
LLM_PROVIDER=ollama

# The narration signature will always use the correct model name (MODEL_NAME) based on the provider and model set above.
# You do not need to set MODEL_NAME manually; it is determined automatically from LLM_MODEL, OLLAMA_MODEL, or VLLM_MODEL.

# Use a simple LLM prompt (for small local models)
SIMPLE_LLM_PROMPT=0

# Production domain for web UI (used by frontend scripts)
PRODUCTION_DOMAIN=nordpool-predict-fi.web.app

# Timezone for the container (default: Europe/Helsinki)
TZ=Europe/Helsinki
# You can override this in your .env.local or docker-compose.yaml if needed.
# Example: TZ=UTC
